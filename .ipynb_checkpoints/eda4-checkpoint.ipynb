{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf47b3b0-ba3b-487f-9c75-9b16de1cf6c5",
   "metadata": {},
   "source": [
    "<h1>Object detection exercise</h1>\n",
    "\n",
    "This exercise attempts to demonstrate if a Convolutional Neural Network (CNN) is able to detect objects within a picture. For a simple example, two classes will be considered: Cars and pedestrians. For eda2, it is a continuation from eda1, but a video file shall be used for test, instead of a static image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e50c6-782f-438e-8490-5cb4b9f95714",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "\n",
    "- pedestrian_and_car.yaml is the file containing directory pathways to train-val-test and the number of classes\n",
    "\n",
    "- yolov5s.pt is the pre-trained weight; small weight chosen for faster training time and less use of memory resources\n",
    "\n",
    "- epochs is the number of training epochs; test images with cars and/or pedestrians are variously fitted after 10, 30 and 50 epochs\n",
    "\n",
    "- batch refers to training batch size; considering that there are only 40 images used together for training and validation, batch size of 4 is appropriate.\n",
    "\n",
    "- Freeze refers to the layer number taken out from Yolov5 to be used for transfer learning. Yolov5 has a total of 24 layers, freeze 10 means that the first 10 layers of CNN are frozen. The first 10 layers constitutes the \"backbone\" of Yolov5.\n",
    " \n",
    "- Path to the weight files can be found under: yolov5/runs/train/exp\"x\"/weights/best.pt, where x is the a number representing the number of run iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73f718-6700-4fa0-86b6-4623dbe0914e",
   "metadata": {},
   "source": [
    "Run 5: 50 epochs\n",
    "\n",
    "Instead of freezing the backbone, lets freeze the entire Yolo layer, except for the final layer to verify literature findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408fed7c-db7f-48e8-9e15-ecf3964d8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=pedestrian_and_car.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[23], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "git: 'models/CV_Yolo5_2/yolov5' is not a git command. See 'git --help'.\n",
      "YOLOv5 ðŸš€ 2024-2-14 Python-3.8.10 torch-2.2.0+cu121 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.9.cv1.conv.weight\n",
      "freezing model.9.cv1.bn.weight\n",
      "freezing model.9.cv1.bn.bias\n",
      "freezing model.9.cv2.conv.weight\n",
      "freezing model.9.cv2.bn.weight\n",
      "freezing model.9.cv2.bn.bias\n",
      "freezing model.10.conv.weight\n",
      "freezing model.10.bn.weight\n",
      "freezing model.10.bn.bias\n",
      "freezing model.13.cv1.conv.weight\n",
      "freezing model.13.cv1.bn.weight\n",
      "freezing model.13.cv1.bn.bias\n",
      "freezing model.13.cv2.conv.weight\n",
      "freezing model.13.cv2.bn.weight\n",
      "freezing model.13.cv2.bn.bias\n",
      "freezing model.13.cv3.conv.weight\n",
      "freezing model.13.cv3.bn.weight\n",
      "freezing model.13.cv3.bn.bias\n",
      "freezing model.13.m.0.cv1.conv.weight\n",
      "freezing model.13.m.0.cv1.bn.weight\n",
      "freezing model.13.m.0.cv1.bn.bias\n",
      "freezing model.13.m.0.cv2.conv.weight\n",
      "freezing model.13.m.0.cv2.bn.weight\n",
      "freezing model.13.m.0.cv2.bn.bias\n",
      "freezing model.14.conv.weight\n",
      "freezing model.14.bn.weight\n",
      "freezing model.14.bn.bias\n",
      "freezing model.17.cv1.conv.weight\n",
      "freezing model.17.cv1.bn.weight\n",
      "freezing model.17.cv1.bn.bias\n",
      "freezing model.17.cv2.conv.weight\n",
      "freezing model.17.cv2.bn.weight\n",
      "freezing model.17.cv2.bn.bias\n",
      "freezing model.17.cv3.conv.weight\n",
      "freezing model.17.cv3.bn.weight\n",
      "freezing model.17.cv3.bn.bias\n",
      "freezing model.17.m.0.cv1.conv.weight\n",
      "freezing model.17.m.0.cv1.bn.weight\n",
      "freezing model.17.m.0.cv1.bn.bias\n",
      "freezing model.17.m.0.cv2.conv.weight\n",
      "freezing model.17.m.0.cv2.bn.weight\n",
      "freezing model.17.m.0.cv2.bn.bias\n",
      "freezing model.18.conv.weight\n",
      "freezing model.18.bn.weight\n",
      "freezing model.18.bn.bias\n",
      "freezing model.20.cv1.conv.weight\n",
      "freezing model.20.cv1.bn.weight\n",
      "freezing model.20.cv1.bn.bias\n",
      "freezing model.20.cv2.conv.weight\n",
      "freezing model.20.cv2.bn.weight\n",
      "freezing model.20.cv2.bn.bias\n",
      "freezing model.20.cv3.conv.weight\n",
      "freezing model.20.cv3.bn.weight\n",
      "freezing model.20.cv3.bn.bias\n",
      "freezing model.20.m.0.cv1.conv.weight\n",
      "freezing model.20.m.0.cv1.bn.weight\n",
      "freezing model.20.m.0.cv1.bn.bias\n",
      "freezing model.20.m.0.cv2.conv.weight\n",
      "freezing model.20.m.0.cv2.bn.weight\n",
      "freezing model.20.m.0.cv2.bn.bias\n",
      "freezing model.21.conv.weight\n",
      "freezing model.21.bn.weight\n",
      "freezing model.21.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/han/Documents/CV models/CV_Yolo5_2/data/labels/train.cache\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/han/Documents/CV models/CV_Yolo5_2/data/labels/val.cache... \u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.38 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to yolov5/runs/train/exp5/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp5\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1218    0.04766    0.03046         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55    0.00556      0.232    0.00401   0.000761\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1136    0.04542    0.02794         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55    0.00554      0.232     0.0042    0.00129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1184    0.04467    0.02833         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.01      0.438     0.0099      0.002\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1135    0.04704    0.02958         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0121       0.53     0.0148    0.00365\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G     0.1029    0.04501    0.02864         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0163      0.712     0.0342    0.00744\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G     0.1097    0.05051    0.02993         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0175      0.768     0.0516      0.013\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G     0.1058    0.04954     0.0275         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0174      0.772     0.0797     0.0217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G     0.1014    0.04555    0.02724         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0325      0.824      0.187     0.0495\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G     0.1001    0.04641    0.02674         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.153      0.551      0.281     0.0634\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G    0.09449    0.04837    0.02547         38        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.153      0.551      0.281     0.0634\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.09437    0.04651    0.02494         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.42      0.511      0.478      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.09251    0.05121     0.0238         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.42      0.511      0.478      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.08555    0.04141    0.02372         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.534      0.526       0.52      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.08432    0.05027    0.02217         33        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.534      0.526       0.52      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G     0.0815    0.04324    0.02449         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.483      0.392      0.424      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G    0.08692     0.0503    0.02289         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.483      0.392      0.424      0.108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.08835    0.04635    0.02256         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.213      0.228       0.18     0.0446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G    0.08914      0.045    0.02146         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.213      0.228       0.18     0.0446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G    0.08585    0.04979    0.02113         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.371      0.284      0.324      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.07831    0.04366    0.01964         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.371      0.284      0.324      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G    0.08537    0.04806    0.02027         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.507      0.451      0.475      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G    0.08783    0.04775    0.01937         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.507      0.451      0.475      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G    0.08381    0.04607    0.01965         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.403      0.625      0.501      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G    0.07786    0.04584    0.01822         35        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.403      0.625      0.501      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.08238    0.04559    0.02064         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.386      0.487      0.431      0.134\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G    0.08232    0.04205    0.01797         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.386      0.487      0.431      0.134\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.07187    0.03709    0.01569         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.499      0.574      0.619      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G    0.07643    0.04083    0.01827         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.499      0.574      0.619      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G    0.08241     0.0479    0.01774         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.545      0.672      0.605      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G    0.08333     0.0434    0.01896         42        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.545      0.672      0.605      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49         0G    0.08361    0.03932    0.01791         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.451      0.645      0.602      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G    0.08223    0.04403     0.0167         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.451      0.645      0.602      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G    0.07259    0.03618    0.01893         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.582      0.689      0.633      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G    0.07537    0.04558      0.015         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.582      0.689      0.633      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49         0G    0.07263    0.04216    0.01513         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.667      0.641      0.715      0.289\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G    0.07823    0.03861    0.01673         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.667      0.641      0.715      0.289\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.07822    0.04333      0.017         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.75      0.693      0.823      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G    0.07856    0.04368    0.01772         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.75      0.693      0.823      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G    0.07773    0.05057     0.0156         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.705      0.722      0.791      0.354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G    0.07598    0.04282     0.0161         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.705      0.722      0.791      0.354\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G    0.07515    0.04416    0.01543         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.638      0.727      0.709      0.303\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G    0.07047     0.0395    0.01395         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.638      0.727      0.709      0.303\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G    0.07916     0.0435    0.01688         37        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.703      0.764      0.767      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G    0.06563    0.03783    0.01627         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.703      0.764      0.767      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G    0.07346    0.03972    0.01508         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.765      0.798      0.838      0.387\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G    0.07266    0.03722    0.01829         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.765      0.798      0.838      0.387\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G    0.07167    0.03412    0.01393         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.748        0.8      0.824      0.355\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.07652    0.04379    0.01587         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.748        0.8      0.824      0.355\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.06827    0.03805    0.01617         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.73      0.802      0.814      0.361\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G    0.07817    0.03668    0.01655         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.73      0.802      0.814      0.361\n",
      "\n",
      "50 epochs completed in 0.063 hours.\n",
      "Optimizer stripped from yolov5/runs/train/exp5/weights/last.pt, 14.4MB\n",
      "Optimizer stripped from yolov5/runs/train/exp5/weights/best.pt, 14.4MB\n",
      "\n",
      "Validating yolov5/runs/train/exp5/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.764      0.798      0.838      0.388\n",
      "                   car          8         26      0.715      0.769      0.808       0.42\n",
      "            pedestrian          8         29      0.813      0.828      0.868      0.356\n",
      "Results saved to \u001b[1myolov5/runs/train/exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 yolov5/train.py --data pedestrian_and_car.yaml --weights yolov5s.pt --epochs 50 --batch 4 --freeze 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249ebd7-a2bb-44c3-8cb6-357cf1813cb7",
   "metadata": {},
   "source": [
    "<h2>Test</h2>\n",
    "\n",
    "- Path to the output can be found under: yolov5/runs/detect/exp<x>/weights/best.pt, where x is the a number representing the number of output iteration\n",
    "\n",
    "- Select best.pt for best performing model, or last.pt for output saved from final epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c02c06-d05f-4960-afb8-1419235605f1",
   "metadata": {},
   "source": [
    "From run5; 23 layers of Yolov5 was used for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949c1030-d55b-4aea-80f6-3cad2b5f9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/exp5/weights/best.pt'], source=yolov5/data/images/, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "git: 'models/CV_Yolo5_2/yolov5' is not a git command. See 'git --help'.\n",
      "YOLOv5 ðŸš€ 2024-2-14 Python-3.8.10 torch-2.2.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/3 /home/han/Documents/CV models/CV_Yolo5_2/yolov5/data/images/T1.jpg: 480x640 1 pedestrian, 70.4ms\n",
      "image 2/3 /home/han/Documents/CV models/CV_Yolo5_2/yolov5/data/images/T2.jpg: 352x640 2 cars, 3 pedestrians, 51.8ms\n",
      "image 3/3 /home/han/Documents/CV models/CV_Yolo5_2/yolov5/data/images/T3.jpg: 448x640 3 cars, 1 pedestrian, 58.3ms\n",
      "Speed: 0.4ms pre-process, 60.2ms inference, 0.6ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5/runs/detect/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 yolov5/detect.py --weights yolov5/runs/train/exp5/weights/best.pt --conf 0.40 --iou 0.50 --source yolov5/data/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3394c-d1a8-43c2-80f6-b10c0288f7d0",
   "metadata": {},
   "source": [
    "<h2>Discussion</h2>\n",
    "\n",
    "\n",
    "Observations:\n",
    "\n",
    "- From run 5: Compared to transfer learning with backbone applied, using the full Yolov5 returns a slightly lower mAP score. However, the mAP score for both classes are closer together. In particular, Recall score for cars has improved from 0.577 to 0.769 with 23 transfer layers used\n",
    "\n",
    "- Cars with lower confidence scores are not detected, but for objects that are detected, they show slightly higher confidence scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeabb01-9e33-452e-8087-8a0c49b0c0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
