{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf47b3b0-ba3b-487f-9c75-9b16de1cf6c5",
   "metadata": {},
   "source": [
    "<h1>Object detection exercise</h1>\n",
    "\n",
    "This exercise attempts to demonstrate if a Convolutional Neural Network (CNN) is able to detect objects within a picture. For a simple example, two classes will be considered: Cars and pedestrians. For eda2, it is a continuation from eda1, but a video file shall be used for test, instead of a static image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e50c6-782f-438e-8490-5cb4b9f95714",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "\n",
    "Notes:\n",
    "\n",
    "- pedestrian_and_car.yaml is the file containing directory pathways to train-val-test and the number of classes\n",
    "\n",
    "- yolov5s.pt is the pre-trained weight; small weight chosen for faster training time and less use of memory resources\n",
    "\n",
    "- epochs is the number of training epochs; test images with cars and/or pedestrians are variously fitted after 10, 30 and 50 epochs\n",
    "\n",
    "- batch refers to training batch size; considering that there are only 40 images used together for training and validation, batch size of 4 is appropriate.\n",
    "\n",
    "- Freeze refers to the layer number taken out from Yolov5 to be used for transfer learning. Yolov5 has a total of 24 layers, freeze 10 means that the first 10 layers of CNN are frozen. The first 10 layers constitutes the \"backbone\" of Yolov5.\n",
    " \n",
    "- Path to the weight files can be found under: yolov5/runs/train/exp\"x\"/weights/best.pt, where x is the a number representing the number of run iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73f718-6700-4fa0-86b6-4623dbe0914e",
   "metadata": {},
   "source": [
    "Run 4: 50 epochs\n",
    "\n",
    "For this run, yolov5s6 is used instead of yolov5 to compare which model performs better with transfer learning - Performance reference can be found at: https://github.com/ultralytics/yolov5#inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408fed7c-db7f-48e8-9e15-ecf3964d8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s6.pt, cfg=, data=pedestrian_and_car.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[10], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "git: 'models/CV_Yolo5_2/yolov5' is not a git command. See 'git --help'.\n",
      "YOLOv5 ðŸš€ 2024-2-14 Python-3.8.10 torch-2.2.0+cu121 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt to yolov5s6.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.8M/24.8M [00:00<00:00, 60.7MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1    885504  models.common.Conv                      [256, 384, 3, 2]              \n",
      "  8                -1  1    665856  models.common.C3                        [384, 384, 1]                 \n",
      "  9                -1  1   1770496  models.common.Conv                      [384, 512, 3, 2]              \n",
      " 10                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      " 11                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 12                -1  1    197376  models.common.Conv                      [512, 384, 1, 1]              \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    813312  models.common.C3                        [768, 384, 1, False]          \n",
      " 16                -1  1     98816  models.common.Conv                      [384, 256, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 20                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 24                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 27                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  1    715008  models.common.C3                        [512, 384, 1, False]          \n",
      " 30                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   1313792  models.common.C3                        [768, 512, 1, False]          \n",
      " 33  [23, 26, 29, 32]  1     26964  models.yolo.Detect                      [2, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [128, 256, 384, 512]]\n",
      "Model summary: 281 layers, 12326164 parameters, 12326164 gradients, 16.3 GFLOPs\n",
      "\n",
      "Transferred 451/459 items from yolov5s6.pt\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.9.conv.weight\n",
      "freezing model.9.bn.weight\n",
      "freezing model.9.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 75 weight(decay=0.0), 79 weight(decay=0.0005), 79 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/han/Documents/CV models/CV_Yolo5_2/data/labels/train.cache\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/han/Documents/CV models/CV_Yolo5_2/data/labels/val.cache... \u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.08 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to yolov5/runs/train/exp4/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1myolov5/runs/train/exp4\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/49         0G     0.1166    0.03939    0.02867         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55    0.00293      0.129    0.00366    0.00112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/49         0G     0.1138    0.03843    0.02804         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55    0.00291      0.129    0.00462    0.00151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       2/49         0G     0.1109    0.03879    0.02856         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0197      0.152    0.00977    0.00268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       3/49         0G     0.1023    0.04441    0.02771         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0216      0.167     0.0101    0.00227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       4/49         0G    0.09639    0.04203    0.02595         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0233      0.169     0.0137    0.00295\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       5/49         0G    0.09324    0.04961    0.02548         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0182      0.129     0.0139    0.00214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       6/49         0G    0.09354    0.04733    0.02535         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0104      0.455     0.0164    0.00435\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       7/49         0G    0.08725    0.04536    0.02468         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0585      0.275     0.0446    0.00992\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       8/49         0G    0.08787     0.0455    0.02465         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0615      0.438     0.0655     0.0171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       9/49         0G    0.07962     0.0479    0.02258         38        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0615      0.438     0.0655     0.0171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      10/49         0G    0.07844    0.04648     0.0232         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0813      0.445     0.0835     0.0175\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      11/49         0G    0.07741    0.05196    0.02143         32        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0813      0.445     0.0835     0.0175\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      12/49         0G    0.07441    0.04389    0.02217         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.176       0.56      0.167     0.0459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      13/49         0G    0.06965     0.0505    0.02267         33        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.176       0.56      0.167     0.0459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      14/49         0G    0.07108    0.04425     0.0223         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0886       0.58     0.0909     0.0248\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      15/49         0G     0.0771    0.04624    0.02132         47        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55     0.0886       0.58     0.0909     0.0248\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      16/49         0G    0.07535    0.04229    0.02164         20        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.214       0.28      0.288      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      17/49         0G    0.07256    0.04183    0.01993         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.214       0.28      0.288      0.111\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      18/49         0G    0.07073    0.04624    0.02121         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.352      0.404      0.332      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      19/49         0G    0.06684    0.04239    0.01958         21        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.352      0.404      0.332      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      20/49         0G    0.06643    0.04569    0.02043         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.394      0.508      0.445      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      21/49         0G    0.06561    0.04395    0.01943         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.394      0.508      0.445      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      22/49         0G    0.06241    0.04281    0.02078         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.356      0.382      0.328      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      23/49         0G    0.05971    0.04263    0.01961         35        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.356      0.382      0.328      0.114\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      24/49         0G    0.06192    0.03981     0.0202         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.552      0.518      0.521      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      25/49         0G    0.06326    0.03663    0.01893         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.552      0.518      0.521      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      26/49         0G    0.06017    0.03443    0.01923         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.447        0.2      0.254     0.0589\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      27/49         0G      0.062    0.03586    0.02111         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.447        0.2      0.254     0.0589\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      28/49         0G    0.06229    0.04379    0.01794         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.594      0.485      0.555      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      29/49         0G    0.06077    0.04011    0.01933         42        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.594      0.485      0.555      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      30/49         0G    0.05736    0.03717     0.0187         25        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.518      0.487      0.542      0.185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      31/49         0G    0.05663    0.03915    0.01732         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.518      0.487      0.542      0.185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      32/49         0G    0.05496    0.03191    0.01912         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.616      0.509      0.546       0.18\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      33/49         0G    0.05701    0.04071     0.0181         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.616      0.509      0.546       0.18\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      34/49         0G    0.05567    0.03557    0.01781         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.693      0.535      0.647      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      35/49         0G    0.05408    0.03465    0.01939         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.693      0.535      0.647      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      36/49         0G    0.05253    0.03888    0.01816         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.591      0.507      0.615      0.291\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      37/49         0G    0.05163    0.03839    0.01827         39        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.591      0.507      0.615      0.291\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      38/49         0G     0.0514    0.04222    0.01621         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.615      0.562      0.637       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      39/49         0G    0.04855    0.03634    0.01734         28        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.615      0.562      0.637       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      40/49         0G    0.04967    0.03753    0.01705         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.63      0.562      0.656      0.278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      41/49         0G    0.04843    0.03631    0.01733         27        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55       0.63      0.562      0.656      0.278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      42/49         0G    0.05132    0.03793    0.01653         37        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.704      0.588      0.728      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      43/49         0G     0.0474    0.03185    0.01769         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.704      0.588      0.728      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      44/49         0G    0.04881     0.0345    0.01695         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.649      0.695       0.74      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      45/49         0G     0.0456    0.03566    0.01561         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.649      0.695       0.74      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      46/49         0G    0.04596    0.03102    0.01646         26        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.645      0.682      0.738      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      47/49         0G    0.04816    0.03831    0.01719         34        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.645      0.682      0.738      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      48/49         0G    0.04467    0.03258    0.01867         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.663      0.691      0.749      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "      49/49         0G    0.04861    0.03368    0.01768         13        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.663      0.691      0.749      0.345\n",
      "\n",
      "50 epochs completed in 0.093 hours.\n",
      "Optimizer stripped from yolov5/runs/train/exp4/weights/last.pt, 25.1MB\n",
      "Optimizer stripped from yolov5/runs/train/exp4/weights/best.pt, 25.1MB\n",
      "\n",
      "Validating yolov5/runs/train/exp4/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 12312052 parameters, 0 gradients, 16.1 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all          8         55      0.663      0.691       0.75      0.346\n",
      "                   car          8         26      0.665      0.692      0.774      0.387\n",
      "            pedestrian          8         29      0.661       0.69      0.725      0.306\n",
      "Results saved to \u001b[1myolov5/runs/train/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 yolov5/train.py --data pedestrian_and_car.yaml --weights yolov5s6.pt --epochs 50 --batch 4 --freeze 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a49f3-b0c1-4057-b03b-8953f331640a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4249ebd7-a2bb-44c3-8cb6-357cf1813cb7",
   "metadata": {},
   "source": [
    "<h2>Test</h2>\n",
    "\n",
    "- Path to the output can be found under: yolov5/runs/detect/exp<x>/weights/best.pt, where x is the a number representing the number of output iteration\n",
    "\n",
    "- Select best.pt for best performing model, or last.pt for output saved from final epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c02c06-d05f-4960-afb8-1419235605f1",
   "metadata": {},
   "source": [
    "From run4 (yolov5s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949c1030-d55b-4aea-80f6-3cad2b5f9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/exp4/weights/best.pt'], source=yolov5/data/images/, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "git: 'models/CV_Yolo5_2/yolov5' is not a git command. See 'git --help'.\n",
      "YOLOv5 ðŸš€ 2024-2-14 Python-3.8.10 torch-2.2.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 12312052 parameters, 0 gradients, 16.1 GFLOPs\n",
      "image 1/3 /home/han/Documents/CV models/CV_Yolo5_2/yolov5/data/images/T1.jpg: 512x640 1 pedestrian, 80.7ms\n",
      "image 2/3 /home/han/Documents/CV models/CV_Yolo5_2/yolov5/data/images/T2.jpg: 384x640 2 cars, 1 pedestrian, 60.6ms\n",
      "image 3/3 /home/han/Documents/CV models/CV_Yolo5_2/yolov5/data/images/T3.jpg: 448x640 1 car, 1 pedestrian, 57.1ms\n",
      "Speed: 0.4ms pre-process, 66.1ms inference, 0.7ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5/runs/detect/exp6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 yolov5/detect.py --weights yolov5/runs/train/exp4/weights/best.pt --conf 0.40 --iou 0.50 --source yolov5/data/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3394c-d1a8-43c2-80f6-b10c0288f7d0",
   "metadata": {},
   "source": [
    "<h2>Discussion</h2>\n",
    "\n",
    "\n",
    "Observations:\n",
    "\n",
    "- Between yolov5s and yolov5s6, yolov5 seems to perform better with better mAP scores, and also more objects detected correctly. All other parameters - confidence threshold, IOU, batch size, number of training epochs are kept constant for comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeabb01-9e33-452e-8087-8a0c49b0c0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
